model_name_or_path: ""
adapter_name_or_path:
finetuning_type: lora

stage: "dpo"
do_train: true
do_eval: true
finetuning_type: "lora"
lora_target: "all"
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.2
pref_beta: 0.2
pref_loss: "sigmoid"  
pref_ftx: 0.1


dataset_dir: ""
dataset: ""
template: "llama3"
cutoff_len: 2048
max_samples: 33612
overwrite_cache: true
preprocessing_num_workers: 16


output_dir: ""
logging_steps: 5
save_steps: 100
plot_loss: true
overwrite_output_dir: true


per_device_train_batch_size: 4
gradient_accumulation_steps: 8
learning_rate: 5.0e-06
num_train_epochs: 4.0
lr_scheduler_type: "cosine"
warmup_steps: 250
max_grad_norm: 1.0
bf16: true
ddp_timeout: 180000000
optim: "adamw_torch"
packing: false


val_size: 0.1
per_device_eval_batch_size: 4
eval_strategy: "steps"
eval_steps: 100



